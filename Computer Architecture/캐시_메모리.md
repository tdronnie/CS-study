# 캐시_메모리

속도가 빠른 장치와 느린 장치 간 속도 차이로 인한 병목 현상을 줄이기 위한 메모리이다. CPU의 성능향상을 위한 장치이다. 

## 속도 차이로 인한 병목현상?

![image](https://user-images.githubusercontent.com/69182630/235431774-e7c7f2bb-af22-44bc-8eb5-17c4f38833bb.png)

CPU와 메모리간 데이터 전송량은 매우 많은데 CPU 속도가 상대적으로 메모리보다 빠르다. 따라서 메모리에서 병목현상이 발생하기 쉽다.

자세히 말하자면 CPU가 메모리 주소를 지정하게 되면 메모리 안의 해당 주소의 데이터를 반환하거나 접근하기 위한 시간이 걸린다. 여기서 CPU가 메모리보다 속도가 빠르기 때문에 메모리에서 병목현상이 발생하게 되는 것이다.

병목현상을 줄이기 위해 캐시 메모리가 도입된다. 캐시의 목적은 자주 사용하는 데이터를 빠르게 가져오기 위함이다. 이는 실제적으로 CPU와 물리적으로 더 가깝게 위치하고 데이터를 저장하고 불러오기 위한 전기신호의 속도가 매우 빠르다.
또 작은 용량을 가지기 때문에 상대적으로 접근시간이 메모리보다는 빠를 수 밖에 없다.


---
## 캐시 메모리 계층구조

![image](https://user-images.githubusercontent.com/69182630/235449962-05d5e1c3-e16c-41c5-a3d2-5deb0f624262.png)
메모리와 캐시는 일종의 데이터 접근 계층을 이루어서 CPU가 데이터 요청 시 빠르게 반환 할 수 있도록 설계된다.

CPU - 내부 캐시 - 외부 캐시 - 메모리 (계층구조 순서로 데이터를 참조한다.)

- 내부 캐시 : CPU 내부에 위치하고 가장 가까운 위치에 배치되므로 최소한의 지연시간을 가진다. 데이터에 접근을 빠르게 해준다.
- 외부 캐시 : CPU 외부에 위치하지만 내부 캐시보다는 빠르게 데이터에 접근 가능하다. 데이터 전송을 줄이는데에 주로 사용된다.

내부 캐시는 L1, L2, L3캐시 정도로 구분된다.(속도와 크기에 따라 분류)

L1이 가장 크기가 작고 CPU와 가깝다. 속도가 가장 빠르다. 따라서 가장 자주 사용되는 데이터를 저장한다.

L2는 CPU와 DRAM 사이에 존재하고 L3는 보통 메인보드에 존재한다. L1에 저장하지 못한 데이터들이 저장되고 여기까지가 SRAM

외부 캐시는 L4캐시, 메인메모리, (디스크 캐시), 디스크 등으로 구분된다.

L4는 용량이 매우 크고 느리지만 내부 캐시에 저장하지 못한 데이터를 저장한다. 메인메모리는 용량이 더 크고 내부, 외부 캐시에 저장하지 못한 데이터를 저장한다.
디스크가 가장 큰 용량을 가지고 매우 느리지만 메인메모리나 캐시에 저장하지 못한 데이터를 장기간 보관할 수 있다.

캐시와 메모리, 하드디스크는 위치, 용량, 속도 등에서 단계적으로 나누어진다.

---
## 캐시 작동원리

캐시는 **지역성**을 가지기 때문에 CPU가 데이터 요청 시 이전에 요청했던 데이터 블록을 함께 가져온다.
따라서 다음에 연관된 요청할 데이터가 이미 데이터 블록에 위치할 가능성이 생기기 때문에 접근시간이 더욱 빨라질 수도 있다.

- 시간 지역성 : 직전이나 참조된 데이터는 짧은 시간안에 또 사용될 수 있다는 특징
- 공간 지역성 : 공간 상 붙어있는 데이터를 연속으로 참조 시 짧은 시간안에 근처에 있는 데이터가 또 사용될 수 있다는 특징

이런 특징을 사용해서 데이터 블록을 활용한다.

데이터를 요청할 때 캐시에서 찾게 된다면 Cache Hit, DRAM까지 가서 찾아오면 Cache Miss

---
## Cache Miss

1. Cold Miss

   접근하려는 데이터의 주소를 처음 요청했을 때 발생하는 미스

2. Conflict Miss

   캐시의 크기가 작아서 캐시 라인이 충돌하는 경우에 발생하는 미스

   캐시는 블록으로 구성되는데 블록은 세트로 그룹화된다.
   만약 블록 수가 세트 수보다 많을 경우 하나의 세트에 두개 이상의 블록이 들어가게 되는데 이때 같은 세트에 접근을 빈번하게 하게 된다면 해당 세트안에 있는 블록들끼리 경쟁이 일어난다.

3. Capacity Miss

   캐시 메모리 공간 부족으로 발생하는 미스
   
---
## 캐시 조직 방식에 따른 작동

1. Direct Mapped Cache  

   이름과 같이 직접 매핑하는 방식으로 DRAM의 여러주소가 캐시 메모리의 한 주소와 연결된다.
   
   한 주소와 연결된다는 것은 결국 캐시 안의 모든 블록이 하나의 세트 안에 할당된다는 것이고
   캐시 라인의 인덱스 비트가 DRAM의 물리적 주소의 일부분을 사용하고 해당 물리적 주소를 캐시라인의 번호로 바꿔주므로써 캐시와 DRAM을 연결시킨다.

   구현이 간단하고 검색이 빠르지만 유일한 캐시 메모리의 라인에 접근하므로 충돌이 발생 위험이 크다. 세트 내 confilct miss도 일어날 수 있다. 따라서 간단한 캐시 시스템에 적합하다.

   (DRAM주소 일부 사용하는 인덱스 비트) 특정캐시라인으로 변환작업 필요! -(유일한 캐시라인)


2. Fully Associative Cache

   비어있는 캐시 메모리가 있으면 마음대로 주소를 저장하는 방식이다. 저장할 때는 단순히 비어있는 곳 나올 때까지 찾고 저장하면 되지만 데이터를 찾을 때는 선형적으로 찾아야한다.

   검색 시간은 다른 조직 방식에 비해서는 오래걸리지만 충돌 미스가 발생할 가능성은 거의 없다. 인덱스 필드를 사용하지 않고 DRAM의 전체 비트를 태그로 사용하기 때문에 위치 상 자유로우면서 캐시 안의 블록을 거의 일대일대응으로 검색할 수 있다.

   (DRAM전체비트 사용하는 태그)-(블럭)-(블럭)-...


3. Set Associative Cache

   Direct + Fully, Direct와 동일하게 인덱스 비트를 사용한다. 다른점은 캐시가 두개 이상의 캐시라인을 가지는 것이다. 따라서 물리적 주소와 연결할 때 여러 라인 중 하나를 선택해야 한다.
   여기서! 캐시라인의 태그 필드를 동시에 검색하고 일치하는 태그가 있는 캐시라인 중 하나를 선택한다. 없으면 미스 발생
   
   인덱스 비트를 사용하고 태그 필드를 동시에 검색한다는 것에서 Fully보다 적은 시간이 들고 충돌 위험도 낮다. Direct보다 유연하다.

   2-way, 4-way등 여러가지 세트구성이 있다. 충돌 미스와 검색시간을 고려해서 데이터의 균형을 유지하면서도 비용을 최소화할 수 있도록 구성해야 한다.
   
   

### 참조

https://github.com/gyoogle/tech-interview-for-developer/blob/master/Computer%20Science/Computer%20Architecture/%EC%BA%90%EC%8B%9C%20%EB%A9%94%EB%AA%A8%EB%A6%AC(Cache%20Memory).md